{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Denoising Autoencoder\n\nThis is a torch implementation of the same architecture I used for the 1st place submission to TPS June 2022. For the sake of having the notebook run fast, I am using a small model. For the actual submission, I used a larger model and averaged 3 model runs (in torch and in tensorflow).","metadata":{}},{"cell_type":"code","source":"import math\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom torch import nn\n\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-29T11:56:27.907668Z","iopub.execute_input":"2022-06-29T11:56:27.908406Z","iopub.status.idle":"2022-06-29T11:56:27.914921Z","shell.execute_reply.started":"2022-06-29T11:56:27.908368Z","shell.execute_reply":"2022-06-29T11:56:27.913965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:56:27.952533Z","iopub.execute_input":"2022-06-29T11:56:27.952811Z","iopub.status.idle":"2022-06-29T11:56:40.138211Z","shell.execute_reply.started":"2022-06-29T11:56:27.952786Z","shell.execute_reply":"2022-06-29T11:56:40.137118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create lists of features\nfeatures = list()\n\nfor f in ('F_1', 'F_2', 'F_3', 'F_4'):\n    features.append([col for col in data if col[:3]==f])\n    \nf1_col, f2_col, f3_col, f4_col = features\n\nfeatures = [f for l in features for f in l]","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:56:40.140265Z","iopub.execute_input":"2022-06-29T11:56:40.140659Z","iopub.status.idle":"2022-06-29T11:56:40.149519Z","shell.execute_reply.started":"2022-06-29T11:56:40.140622Z","shell.execute_reply":"2022-06-29T11:56:40.148478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define masks\n\n\n# Mask for training -> binomial + base one per row\ndef random_mask(shape, binomial_p=0.05):\n    n, k = shape\n    mask = np.ones((n, k))\n    # Set minimum one random per row\n    mask[(\n        np.arange(n),\n        np.random.randint(0, k, n)\n    )] = 0\n    # Add binomial probability as well\n    b_mask = np.random.binomial(1, 1-binomial_p, (n, k))\n    return mask * b_mask\n\n\n# Mask for validation - fixed n missing per row\ndef mask_n_rows(shape, n_missing):\n    n, k = shape\n    s = np.arange(k)[np.newaxis, :].repeat(n, axis=0).reshape(n, k)\n    idx = np.random.rand(n, k).argsort(1)[:,:n_missing]\n    col_idx = np.take_along_axis(s, idx, axis=1).ravel()\n    row_idx = np.arange(n).repeat(n_missing)\n    \n    mask = np.ones((n, k))\n    mask[(\n        row_idx,\n        col_idx\n    )] = 0\n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:56:40.150879Z","iopub.execute_input":"2022-06-29T11:56:40.151264Z","iopub.status.idle":"2022-06-29T11:56:40.161258Z","shell.execute_reply.started":"2022-06-29T11:56:40.151228Z","shell.execute_reply":"2022-06-29T11:56:40.160314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the train and validation set\n\nis_missing_bool = data[f4_col].isna().sum(axis=1) > 0\n\n# Define subsets of the data with row-wise missing values\nX_complete = data.loc[~is_missing_bool, f4_col].values\nX_missing = data.loc[is_missing_bool, f4_col].values\n\n# Split data that has no missing to use for eval set\nX_train_complete, X_valid = train_test_split(X_complete, random_state=6) # Same as previous\n\n# Build train set from complete and missing data\nX_train = np.concatenate([X_train_complete, X_missing], axis=0)\n\n# Mask to show train values that have been imputed\nsrce_nan_train = np.concatenate([\n    np.zeros(X_train_complete.shape),\n    data.loc[is_missing_bool, f4_col].isna().astype(np.uint8).values\n])\n\n# Feature scaling\nscaler = StandardScaler()\n\nscaler.fit(data[f4_col].values)\n\nX_train = np.nan_to_num(scaler.transform(X_train), 0.0)\nX_valid = scaler.transform(X_valid)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:56:40.164064Z","iopub.execute_input":"2022-06-29T11:56:40.165069Z","iopub.status.idle":"2022-06-29T11:56:41.706132Z","shell.execute_reply.started":"2022-06-29T11:56:40.165031Z","shell.execute_reply":"2022-06-29T11:56:41.704938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build Model\n\nclass MLP(nn.Module):\n# Dense layer with layer normalization and mish activation\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.dense = nn.Linear(input_size, output_size)\n        self.act = nn.Mish()\n        self.layernorm = nn.LayerNorm(output_size, eps=1e-6)\n        \n    def forward(self, x):\n        x = self.dense(x)\n        x = self.act(x)\n        return self.layernorm(x)\n    \n# Msked autoencoder model\nclass MaskedAutoencoder(nn.Module):\n    def __init__(self, n_columns, emb_dim=16,\n                 units=[512, 512, 512, 512, 512, 128]):\n        super().__init__()\n        self.n_columns = n_columns\n\n        # Embedding\n        self.inp_proj = nn.Linear(1, emb_dim)\n        self.mask_proj = nn.Linear(1, emb_dim)\n        self.emb_norm = nn.LayerNorm(n_columns * emb_dim, eps=1e-6)\n        \n        # MLP with skip connection\n        self.mlp_layers = nn.ModuleList([])\n        for i in range(len(units)):\n            if i==0:\n                input_size = n_columns * emb_dim\n            elif i==1:\n                input_size = n_columns * emb_dim + units[0]\n            else:\n                input_size = units[i-1] + units[i-2]\n            output_size = units[i]\n            self.mlp_layers.append(\n                MLP(input_size=input_size, output_size=output_size)\n            )\n                \n        self.final_dense = nn.Linear(units[-1] + units[-2], self.n_columns)\n        \n    def forward(self, inputs:torch.Tensor, mask:torch.Tensor):\n        # Embeddings\n        input_embedding = self.inp_proj(torch.unsqueeze(inputs, 2))\n        mask_embedding = self.mask_proj(torch.unsqueeze(1-mask, 2))\n        embedding = input_embedding + mask_embedding\n        embedding = torch.flatten(embedding, 1)\n        x = [self.emb_norm(embedding)]\n        \n        # MLP\n        for i in range(len(self.mlp_layers)):\n            if i==0:\n                z = self.mlp_layers[i](x[0])\n                x.append(z)\n            else:\n                z = torch.cat((x[-1], x[-2]), 1)\n                z = self.mlp_layers[i](z)\n                x.append(z)\n                \n        x = torch.cat((x[-1], x[-2]), 1)\n        x = self.final_dense(x)\n        \n        # Output modification - predict only masked values, otherwise use inputs\n        outputs = torch.mul(inputs, mask) + torch.mul(1-mask, x)\n        \n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:56:41.707797Z","iopub.execute_input":"2022-06-29T11:56:41.708191Z","iopub.status.idle":"2022-06-29T11:56:41.727001Z","shell.execute_reply.started":"2022-06-29T11:56:41.708151Z","shell.execute_reply":"2022-06-29T11:56:41.726017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper validation method\ndef validate(model, valid_mask, batch_size=4096):\n    assert valid_mask.shape == X_valid.shape\n    \n    n_batches_valid = X_valid.shape[0] // batch_size + 1\n    \n    model.eval()\n    with torch.no_grad():\n        ps = []\n        for batch in range(n_batches_valid):\n            x = torch.tensor(X_valid[batch * batch_size: (batch+1) * batch_size].astype(np.float32)).to(device)\n            mask = torch.tensor(valid_mask[batch * batch_size: (batch+1) * batch_size].astype(np.float32)).to(device)\n            x_masked = x * mask\n\n            p = model(x_masked, mask).cpu().numpy()\n            ps.append(p)\n\n        p = np.vstack(ps)\n        mask_bool = (1 - valid_mask).astype(bool)\n        rmse = np.sqrt(mean_squared_error(\n            scaler.inverse_transform(p)[mask_bool],\n            scaler.inverse_transform(X_valid)[mask_bool]\n        ))\n        return rmse","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:56:41.729742Z","iopub.execute_input":"2022-06-29T11:56:41.730851Z","iopub.status.idle":"2022-06-29T11:56:41.741079Z","shell.execute_reply.started":"2022-06-29T11:56:41.730808Z","shell.execute_reply":"2022-06-29T11:56:41.740166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function to mask NaNs in the original data\nclass MaskedMSELoss(nn.Module):\n    # Mask should be 1 for masked value, 0 for unmasked value \n    def __init__(self):\n        super().__init__()\n        self.loss = nn.MSELoss(reduction='none')\n    \n    def forward(self, inputs, target, mask):\n        loss = self.loss(inputs, target)\n        return torch.mean(loss * (1 - mask))","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:56:41.742536Z","iopub.execute_input":"2022-06-29T11:56:41.743545Z","iopub.status.idle":"2022-06-29T11:56:41.752298Z","shell.execute_reply.started":"2022-06-29T11:56:41.743505Z","shell.execute_reply":"2022-06-29T11:56:41.751342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining model parameters and learning rate schedule\n\nEPOCHS = 300\nLR_START = 0.001\nLR_END = 0.00005\nBATCH_SIZE = 4096\n\n# This cosine decay function is borrowed from AmbrosM in last month's TPS\ndef cosine_decay(epoch):\n    epochs = EPOCHS\n    lr_start = LR_START\n    lr_end = LR_END\n    if epochs > 1:\n        w = (1 + math.cos(epoch / (epochs-1) * math.pi)) / 2\n    else:\n        w = 1\n    return w * lr_start + (1 - w) * lr_end","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:56:41.753993Z","iopub.execute_input":"2022-06-29T11:56:41.754403Z","iopub.status.idle":"2022-06-29T11:56:41.762022Z","shell.execute_reply.started":"2022-06-29T11:56:41.754365Z","shell.execute_reply":"2022-06-29T11:56:41.760931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build model\n\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_normal_(m.weight)\n        m.bias.data.fill_(0.01)\n\n# Build model\ndevice = 'cuda'\n\n# Final model uses units = [2048, 2048, 2048, 1024, 512, 256, 128], but I use a smaller model for this notebook\nmodel = MaskedAutoencoder(15, units=[512, 512, 512, 512, 512, 256, 128]).to(device)\nmodel.apply(init_weights)\noptimizer = torch.optim.Adam(model.parameters(), lr=1)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=cosine_decay)\nloss_fn = MaskedMSELoss()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:56:41.763779Z","iopub.execute_input":"2022-06-29T11:56:41.764187Z","iopub.status.idle":"2022-06-29T11:56:41.794862Z","shell.execute_reply.started":"2022-06-29T11:56:41.764152Z","shell.execute_reply":"2022-06-29T11:56:41.793898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop\n\n# for epoch in epochs...\n\nnp.random.seed(6)\n\nn = X_train.shape[0]\nbatch_size = 4096\nn_batches = n // batch_size + 1\nindex = np.arange(n)\n\nvalid_per = 5\n\n# Validation Mask\nvalidation_masks = [mask_n_rows(X_valid.shape, i+1) for i in range(5)]\nvalidation_prob = list(data[f4_col].isna().sum(axis=1).value_counts() \\\n    / data.loc[data[f4_col].isna().sum(axis=1)>0, f4_col].isna().sum(axis=1).value_counts().sum())[1:]\n\nc_scores = [np.zeros(EPOCHS) for i in range(len(validation_masks))]\nf_scores = np.zeros(EPOCHS)\n\n# Training loop\nmodel.train()\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1} LR {optimizer.param_groups[0]['lr']}\")\n    \n    np.random.shuffle(index)\n    losses = 0\n    norm_losses = 0\n    for i in tqdm(range(n_batches)):\n        batch_idx = index[i*batch_size:(i+1)*batch_size]\n        # Create batch train data\n        srce_mask = torch.tensor(srce_nan_train[batch_idx].astype(np.float32)).to(device)\n        x = torch.tensor(X_train[batch_idx].astype(np.float32)).to(device)\n        mask_init = torch.tensor(random_mask(x.shape, binomial_p=0.05).astype(np.float32)).to(device)\n        mask = mask_init - srce_mask * mask_init\n        x_masked = x * mask\n\n        # Forward and backward pass\n        optimizer.zero_grad()\n        p = model(x_masked, mask)\n        loss = loss_fn(p, x, srce_mask)\n        loss.backward()\n        optimizer.step()\n        \n        losses += loss # Check\n    scheduler.step()\n        \n        \n    # Validation stepb\n    if (epoch + 1) % valid_per == 0:\n        scores = []\n        for i in range(len(validation_masks)):\n            v = validate(model, validation_masks[i])\n            scores.append(v)\n            c_scores[i][epoch] = v\n            \n        final_score = math.sqrt(sum([scores[i]**2 * validation_prob[i] for i in range(len(scores))]))\n        f_scores[epoch] = final_score\n        \n        for i in range(len(scores)):\n            print(f'RMSE ({i+1} rows) {scores[i]}')\n        print(f'RMSE (TDGP) {final_score}')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:56:41.798594Z","iopub.execute_input":"2022-06-29T11:56:41.798883Z","iopub.status.idle":"2022-06-29T12:12:12.166357Z","shell.execute_reply.started":"2022-06-29T11:56:41.798858Z","shell.execute_reply":"2022-06-29T12:12:12.163856Z"},"trusted":true},"execution_count":null,"outputs":[]}]}